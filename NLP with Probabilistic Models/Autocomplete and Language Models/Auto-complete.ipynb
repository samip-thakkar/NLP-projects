{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will build autocomplete system in this notebook. We will use n-gram as language model to predict the next word in the sentence. We will use twitter data as corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of letters: 3335477\n",
      "First 100 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the data\n",
    "with open('en_US.twitter.txt', 'r', encoding = 'utf8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 100 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like NLP.\n",
      " I like Machine Learning.\n",
      " I like Data Science.\n",
      " \n",
      "['I like NLP.', 'I like Machine Learning.', 'I like Data Science.']\n"
     ]
    }
   ],
   "source": [
    "# Split the data into sentence (Split by \\n)\n",
    "def split_data_to_sentences(data):\n",
    "    \n",
    "    sentences = data.split('\\n')\n",
    "    \n",
    "    #Remove the extra spaces to remove empty line\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if s]\n",
    "    return sentences\n",
    "\n",
    "#Test the function\n",
    "temp_data ='I like NLP.\\n I like Machine Learning.\\n I like Data Science.\\n '\n",
    "print(temp_data)\n",
    "\n",
    "print(split_data_to_sentences(temp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like NLP.', 'I like Machine Learning.', 'I like Data Science.']\n",
      "[['i', 'like', 'nlp', '.'], ['i', 'like', 'machine', 'learning', '.'], ['i', 'like', 'data', 'science', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Split the sentence into tokens (Use nltk's word tokenizer)\n",
    "def split_sentences_to_tokens(sentences):\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        #Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        #Tokenize the sentence\n",
    "        sentence = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        tokenized_sentences.append(sentence)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "#Test the function\n",
    "temp_sentences = ['I like NLP.', 'I like Machine Learning.', 'I like Data Science.']\n",
    "print(temp_sentences)\n",
    "\n",
    "print(split_sentences_to_tokens(temp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like', 'nlp', '.'], ['i', 'like', 'machine', 'learning', '.'], ['i', 'like', 'data', 'science', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Define a function that creates tokens directly from data\n",
    "def tokenize_data(data):\n",
    "    \n",
    "    #Get the sentences from data\n",
    "    sentences = split_data_to_sentences(data)\n",
    "    \n",
    "    #Get the tokenized sentences from sentences\n",
    "    tokenized_sentences = split_sentences_to_tokens(sentences)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "#Test the function\n",
    "temp_data = 'I like NLP.\\n I like Machine Learning.\\n I like Data Science.\\n '\n",
    "print(tokenize_data(temp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'are', 'you', '?', 'btw', 'thanks', 'for', 'the', 'rt', '.', 'you', 'gon', 'na', 'be', 'in', 'dc', 'anytime', 'soon', '?', 'love', 'to', 'see', 'you', '.', 'been', 'way', ',', 'way', 'too', 'long', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize our data\n",
    "tokenized_data = tokenize_data(data)\n",
    "print(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 data are split into 2 train and 1 test set\n",
      "First training sample:\n",
      "['i', 'like', 'machine', 'learning', '.']\n",
      "First test sample\n",
      "['i', 'like', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "#Split data in 80-20 ratio\n",
    "\n",
    "def train_test_split(tokenized_sentences, percentage):\n",
    "    #tokenized_data = tokenize_data(data)\n",
    "    random.seed(42)\n",
    "    random.shuffle(tokenized_sentences)\n",
    "\n",
    "    train_size = int(len(tokenized_sentences) * percentage / 100)\n",
    "    train_data = tokenized_sentences[:train_size]\n",
    "    test_data = tokenized_sentences[train_size:]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "temp_data = 'I like NLP.\\n I like Machine Learning.\\n I like Data Science.\\n '\n",
    "temp_tokenized_sentences = tokenize_data(temp_data)\n",
    "temp_train_data, temp_test_data = train_test_split(temp_tokenized_sentences, 67)\n",
    "print(\"{} data are split into {} train and {} test set\".format(\n",
    "    len(temp_tokenized_sentences), len(temp_train_data), len(temp_test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(temp_train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(temp_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47961 data are split into 38368 train and 9593 test set\n",
      "First training sample:\n",
      "['what', 'little', 'i', 'have', 'goes', 'toward', 'my', 'commute', '.', '$', '4', 'gasoline', '.']\n",
      "First test sample\n",
      "['at', 'least', 'it', 'was', 'ervin', 'santana', \"'s\", 'own', 'wild', 'pitch', 'that', 'kept', 'him', 'from', 'throwing', 'a', 'scorelss', 'no-hitter', '.', 'pretty', 'unique', 'i', 'must', 'say', '.']\n"
     ]
    }
   ],
   "source": [
    "#Split our data into train and test\n",
    "train_data, test_data = train_test_split(tokenized_data, 80)\n",
    "print(\"{} data are split into {} train and {} test set\".format(\n",
    "    len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 3, 'like': 3, 'nlp': 1, '.': 3, 'machine': 1, 'learning': 1, 'data': 1, 'science': 1}\n"
     ]
    }
   ],
   "source": [
    "#Rather than using all the words, we will use only words with a threshold frequency to make efficient computation\n",
    "\n",
    "#Define the function to word counts\n",
    "def word_count(tokenized_sentences):\n",
    "    \n",
    "    word_counts = {}\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "    return word_counts\n",
    "\n",
    "#Test the code\n",
    "# test your code\n",
    "temp_tokenized_sentences = [['i', 'like', 'nlp', '.'], ['i', 'like', 'machine', 'learning', '.'], ['i', 'like', 'data', 'science', '.']]\n",
    "print(word_count(temp_tokenized_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', '.']\n",
      "['i', '.']\n"
     ]
    }
   ],
   "source": [
    "#Define a function to create a vocabulary with words having frequency greater than threshold\n",
    "def create_vocab(tokenized_sentences, threshold):\n",
    "    \n",
    "    #Create a list to store the vocab\n",
    "    vocab = []\n",
    "    \n",
    "    #Get the word counts of all the tokens\n",
    "    word_counts = word_count(tokenized_sentences)\n",
    "    \n",
    "    #Iterate over the dictionary and check the occurence of all words\n",
    "    for word, count in word_counts.items():\n",
    "        \n",
    "        if count >= threshold:\n",
    "            vocab.append(word)\n",
    "    return vocab\n",
    "\n",
    "#Test the function\n",
    "temp_tokenized_sentences = [['i', 'like', 'nlp', '.'], ['i', 'love', 'machine', 'learning', '.'], ['i', 'like', 'data', 'science', '.']]\n",
    "print(create_vocab(temp_tokenized_sentences, 2))\n",
    "print(create_vocab(temp_tokenized_sentences, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling out of vocabulory words (frequency than threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like', '<unk>', '.'], ['i', '<unk>', '<unk>', '<unk>', '.'], ['i', 'like', '<unk>', '<unk>', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Replace the words with frequency less than threshold with unknown tag\n",
    "\n",
    "def replace_oov_word(tokenized_sentences, vocab, unknown_tag = '<unk>'):\n",
    "    \n",
    "    #Get the set of vocabulary\n",
    "    vocab = set(vocab)\n",
    "    \n",
    "    #Initiate a list to store final sentences\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        replaced_sentence = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            if word in vocab:\n",
    "                replaced_sentence.append(word)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_tag)\n",
    "        \n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    \n",
    "    return replaced_tokenized_sentences\n",
    "\n",
    "#Test the code\n",
    "temp_tokenized_sentences = [['i', 'like', 'nlp', '.'], ['i', 'love', 'machine', 'learning', '.'], ['i', 'like', 'data', 'science', '.']]\n",
    "temp_vocab = create_vocab(temp_tokenized_sentences, 2)\n",
    "\n",
    "print(replace_oov_word(temp_tokenized_sentences, temp_vocab, '<unk>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the preprocessed train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed train data:  [['i', '<unk>', '<unk>', '<unk>', '.'], ['i', '<unk>', '<unk>', '.']]\n",
      "Preprocessed test data:  [['i', '<unk>', '<unk>', '<unk>', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Define the function to preprocess the train and test data\n",
    "def preprocess_data(train_data, test_data, threshold):\n",
    "    \n",
    "    #Get the vocab\n",
    "    vocab = create_vocab(train_data, threshold)\n",
    "    \n",
    "    #Create the replaced train and test data\n",
    "    train_data_replaced = replace_oov_word(train_data, vocab, '<unk>')\n",
    "    test_data_replaced = replace_oov_word(test_data, vocab, '<unk>')\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocab\n",
    "\n",
    "#Test the function\n",
    "temp_tokenized_sentences = [['i', 'like', 'nlp', '.'], ['i', 'love', 'machine', 'learning', '.'], ['i', 'like', 'data', 'science', '.']]\n",
    "temp_train_data, temp_test_data = train_test_split(temp_tokenized_sentences, 67)\n",
    "temp_train_data_replaced, temp_test_data_replaced, temp_vocab = preprocess_data(temp_train_data, temp_test_data, 2)\n",
    "print(\"Preprocessed train data: \", temp_train_data_replaced)\n",
    "print(\"Preprocessed test data: \", temp_test_data_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['what', 'little', 'i', 'have', 'goes', 'toward', 'my', 'commute', '.', '$', '4', 'gasoline', '.']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['at', 'least', 'it', 'was', '<unk>', 'santana', \"'s\", 'own', 'wild', 'pitch', 'that', 'kept', 'him', 'from', 'throwing', 'a', '<unk>', 'no-hitter', '.', 'pretty', 'unique', 'i', 'must', 'say', '.']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['what', 'little', 'i', 'have', 'goes', 'toward', 'my', 'commute', '.', '$']\n",
      "\n",
      "Size of vocabulary: 14859\n"
     ]
    }
   ],
   "source": [
    "#Preprocess our data\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, threshold = 2)\n",
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop n-gram based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "#Define a function that computes the n-grams count\n",
    "def count_n_grams(data, n, start_token = '<s>', end_token = '<e>'):\n",
    "    \n",
    "    #Initialize dictionary to store the counts\n",
    "    n_grams = {}\n",
    "    \n",
    "    for sentence in data:\n",
    "        \n",
    "        #Append start and end tokens\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        #Convert to tuple to store as key\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        if n == 1:\n",
    "            l = len(sentence)\n",
    "        else:\n",
    "            l = len(sentence) - 1\n",
    "            \n",
    "        for i in range(l):\n",
    "            \n",
    "            #Get n-gram\n",
    "            n_gram = sentence[i: i + n]\n",
    "            \n",
    "            #Increase the count in dictionary\n",
    "            if n_gram in n_grams:\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "            \n",
    "    return n_grams\n",
    "\n",
    "#Test the function\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(temp_sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(temp_sentences, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "#Estimate the probability of a n-gram word\n",
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocab_size, k = 1.0):\n",
    "    \n",
    "    #Convert the previous n_gram list to tuple\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    #Get the previous n_gram count\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "    \n",
    "    #Set the denominator\n",
    "    denominator = previous_n_gram_count + k * vocab_size\n",
    "    \n",
    "    #Create the n_plus_1 word \n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    \n",
    "    #Check its count in other dictionary (+1 gram)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "    \n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    #Calculate the probability\n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability\n",
    "\n",
    "#Test the function\n",
    "# test your code\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "\n",
    "temp_unigram_counts = count_n_grams(temp_sentences, 1)\n",
    "temp_bigram_counts = count_n_grams(temp_sentences, 2)\n",
    "temp_tmp_prob = estimate_probability(\"cat\", \"a\", temp_unigram_counts, temp_bigram_counts, len(temp_unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {temp_tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'this': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " 'i': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define a function to estimate the probability of all words\n",
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k = 1.0):\n",
    "    \n",
    "    #Create the list to tuple\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    #Add unknown and end tokens to vocabulary. <s> is not needed as it does not appear as next word\n",
    "    vocabulary += ['<e>' , '<unk>']\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocab_size, k)\n",
    "        probabilities[word] = probability\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "#Test the code as following word 'a'\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "temp_unigram_counts = count_n_grams(temp_sentences, 1)\n",
    "temp_bigram_counts = count_n_grams(temp_sentences, 2)\n",
    "estimate_probabilities(\"a\", temp_unigram_counts, temp_bigram_counts, temp_unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.07692307692307693,\n",
       " 'like': 0.07692307692307693,\n",
       " 'dog': 0.07692307692307693,\n",
       " 'this': 0.15384615384615385,\n",
       " 'is': 0.07692307692307693,\n",
       " 'cat': 0.07692307692307693,\n",
       " 'i': 0.15384615384615385,\n",
       " '<e>': 0.07692307692307693,\n",
       " '<unk>': 0.07692307692307693}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test on bigram and trigram as starting word\n",
    "temp_trigram_counts = count_n_grams(temp_sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], temp_bigram_counts, temp_trigram_counts, temp_unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create count and probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           a  like  dog  this   is  cat    i  <e>  <unk>\n",
       "(this,)  0.0   0.0  1.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(<s>,)   0.0   0.0  0.0   1.0  0.0  0.0  1.0  0.0    0.0\n",
       "(a,)     0.0   0.0  0.0   0.0  0.0  2.0  0.0  0.0    0.0\n",
       "(i,)     0.0   1.0  0.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(like,)  2.0   0.0  0.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(dog,)   0.0   0.0  0.0   0.0  1.0  0.0  0.0  0.0    0.0\n",
       "(is,)    0.0   1.0  0.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(cat,)   0.0   0.0  0.0   0.0  0.0  0.0  0.0  2.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define a function that creates a count matrix\n",
    "def create_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    \n",
    "    #Add end and unknown tokens\n",
    "    vocabulary += ['<e>', '<unk>']\n",
    "    \n",
    "    #Get the n-grams\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    #Mapping n-gram to row\n",
    "    row_index = {n_gram: i for i, n_gram in enumerate(n_grams)}\n",
    "    #Mapping next word to column\n",
    "    col_index = {word: j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    \n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    \n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        \n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        \n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        \n",
    "        count_matrix[i][j] = count\n",
    "    \n",
    "    #Convert matrix to DataFrame\n",
    "    count_matrix = pd.DataFrame(count_matrix, index = n_grams, columns = vocabulary)\n",
    "    return count_matrix\n",
    "\n",
    "#Test the function\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "temp_bigram_counts = count_n_grams(temp_sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(create_count_matrix(temp_bigram_counts, temp_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               a  like  dog  this   is  cat    i  <e>  <unk>\n",
       "(<s>, this)  0.0   0.0  1.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(is, like)   1.0   0.0  0.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(dog, is)    0.0   1.0  0.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(i, like)    1.0   0.0  0.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(<s>, <s>)   0.0   0.0  0.0   1.0  0.0  0.0  1.0  0.0    0.0\n",
       "(<s>, i)     0.0   1.0  0.0   0.0  0.0  0.0  0.0  0.0    0.0\n",
       "(like, a)    0.0   0.0  0.0   0.0  0.0  2.0  0.0  0.0    0.0\n",
       "(a, cat)     0.0   0.0  0.0   0.0  0.0  0.0  0.0  2.0    0.0\n",
       "(this, dog)  0.0   0.0  0.0   0.0  1.0  0.0  0.0  0.0    0.0\n",
       "(cat,)       0.0   0.0  0.0   0.0  0.0  0.0  0.0  2.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check for trigram counts\n",
    "print(\"Trigram counts\")\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "temp_trigram_counts = count_n_grams(temp_sentences, 3)\n",
    "display(create_count_matrix(temp_trigram_counts, temp_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                a      like       dog      this        is       cat         i  \\\n",
       "(this,)  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>,)   0.090909  0.090909  0.090909  0.181818  0.090909  0.090909  0.181818   \n",
       "(a,)     0.090909  0.090909  0.090909  0.090909  0.090909  0.272727  0.090909   \n",
       "(i,)     0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(like,)  0.272727  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(dog,)   0.100000  0.100000  0.100000  0.100000  0.200000  0.100000  0.100000   \n",
       "(is,)    0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(this,)  0.100000  0.100000  \n",
       "(<s>,)   0.090909  0.090909  \n",
       "(a,)     0.090909  0.090909  \n",
       "(i,)     0.100000  0.100000  \n",
       "(like,)  0.090909  0.090909  \n",
       "(dog,)   0.100000  0.100000  \n",
       "(is,)    0.100000  0.100000  \n",
       "(cat,)   0.272727  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define a function to convert the count matrix to the probability matrix\n",
    "def create_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    \n",
    "    #Get the count matrix\n",
    "    count_matrix = create_count_matrix(n_plus1_gram_counts, vocabulary)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix\n",
    "\n",
    "#Test the function\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "temp_bigram_counts = count_n_grams(temp_sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(create_probability_matrix(temp_bigram_counts, temp_unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    a      like       dog      this        is       cat  \\\n",
       "(<s>, this)  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(is, like)   0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(dog, is)    0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(i, like)    0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, <s>)   0.090909  0.090909  0.090909  0.181818  0.090909  0.090909   \n",
       "(<s>, i)     0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(like, a)    0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
       "(a, cat)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(this, dog)  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "(cat,)       0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "\n",
       "                    i       <e>     <unk>  \n",
       "(<s>, this)  0.100000  0.100000  0.100000  \n",
       "(is, like)   0.100000  0.100000  0.100000  \n",
       "(dog, is)    0.100000  0.100000  0.100000  \n",
       "(i, like)    0.100000  0.100000  0.100000  \n",
       "(<s>, <s>)   0.181818  0.090909  0.090909  \n",
       "(<s>, i)     0.100000  0.100000  0.100000  \n",
       "(like, a)    0.090909  0.090909  0.090909  \n",
       "(a, cat)     0.090909  0.272727  0.090909  \n",
       "(this, dog)  0.100000  0.100000  0.100000  \n",
       "(cat,)       0.090909  0.272727  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check for trigram probabilities\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "temp_trigram_counts = count_n_grams(temp_sentences, 3)\n",
    "print(\"Trigram probabilities\")\n",
    "display(create_probability_matrix(temp_trigram_counts, temp_unique_words, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 3.3674\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "#Define a function to calculate perplexity\n",
    "def calculatePerplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k = 1.0):\n",
    "    \n",
    "    #Get the length of n_gram_counts\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    \n",
    "    #Append start and end tags\n",
    "    sentence = ['<s>'] * n + sentence + ['</e>']\n",
    "    \n",
    "    #Convert the sentence to tuple\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    #Get the length of sentence\n",
    "    N = len(sentence)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "    \n",
    "    #Iterate from n to N\n",
    "    for i in range(n, N):\n",
    "        n_gram = sentence[i-n: i]\n",
    "        \n",
    "        word = sentence[i]\n",
    "        \n",
    "        #Get the probability\n",
    "        prob = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k = 1)\n",
    "        \n",
    "        product_pi *= 1 / prob\n",
    "    \n",
    "    perplexity = product_pi ** (1 / float(N))\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "#Test the function\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "\n",
    "temp_unigram_counts = count_n_grams(temp_sentences, 1)\n",
    "temp_bigram_counts = count_n_grams(temp_sentences, 2)\n",
    "\n",
    "\n",
    "temp_perplexity_train1 = calculatePerplexity(temp_sentences[0],\n",
    "                                         temp_unigram_counts, temp_bigram_counts,\n",
    "                                         len(temp_unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {temp_perplexity_train1:.4f}\")\n",
    "\n",
    "temp_test_sentence = ['i', 'like', 'a', 'dog']\n",
    "temp_perplexity_test = calculatePerplexity(temp_test_sentence,\n",
    "                                       temp_unigram_counts, temp_bigram_counts,\n",
    "                                       len(temp_unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {temp_perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an autocomplete system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0769\n"
     ]
    }
   ],
   "source": [
    "#Define a function that suggests the next word based on given text\n",
    "def suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k = 1.0, starts_with = None):\n",
    "    \n",
    "    #Get the length of previous words\n",
    "    n = len((list(n_gram_counts.keys()))[0])\n",
    "    \n",
    "    #Get the last n words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    #Get the probabilities for each word given previous words\n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k = k)\n",
    "    \n",
    "    #Initialize the suggestion word\n",
    "    suggestion = None\n",
    "    \n",
    "    #Initialize the probability to zero\n",
    "    max_prob = 0\n",
    "    \n",
    "    #Iterate over all word in probabilities and get max prob\n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        #Check if start with is given\n",
    "        if starts_with:\n",
    "            \n",
    "            #If the word does not begin with start with, continue\n",
    "            if not word.startswith(starts_with):\n",
    "                continue\n",
    "        \n",
    "        #Update suggestion if prob > max_prob\n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    \n",
    "    return suggestion, max_prob\n",
    "\n",
    "#Test the function\n",
    "# test your code\n",
    "temp_sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "temp_unique_words = list(set(temp_sentences[0] + temp_sentences[1]))\n",
    "\n",
    "temp_unigram_counts = count_n_grams(temp_sentences, 1)\n",
    "temp_bigram_counts = count_n_grams(temp_sentences, 2)\n",
    "\n",
    "temp_previous_tokens = [\"i\", \"like\"]\n",
    "temp_tmp_suggest1 = suggest_word(temp_previous_tokens, temp_unigram_counts, temp_bigram_counts, temp_unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{temp_tmp_suggest1[0]}` with a probability of {temp_tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "# test your code when setting the starts_with\n",
    "temp_tmp_starts_with = 'c'\n",
    "temp_tmp_suggest2 = suggest_word(temp_previous_tokens, temp_unigram_counts, temp_bigram_counts, temp_unique_words, k=1.0, starts_with=temp_tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{temp_tmp_starts_with}`\\n\\tand the suggested word is `{temp_tmp_suggest2[0]}` with a probability of {temp_tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727),\n",
       " ('a', 0.16666666666666666),\n",
       " ('a', 0.07692307692307693),\n",
       " ('a', 0.06666666666666667)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define a function to get multiple suggestions\n",
    "def get_multiple_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k = 1.0, starts_with = None):\n",
    "    \n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    for i in range(model_counts - 1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i + 1]\n",
    "        \n",
    "        suggestion = suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k = k, starts_with = starts_with)\n",
    "        \n",
    "        suggestions.append(suggestion)\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "#Test the function\n",
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_multiple_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get suggestions for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('be', 0.02690861294680346),\n",
       " ('have', 0.00013449899125756557),\n",
       " ('have', 0.00013452613170108295),\n",
       " ('what', 6.726306585054147e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "tmp_suggest4 = get_multiple_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 0.014243065057843016),\n",
       " ('to', 0.0050784856879039705),\n",
       " ('to', 0.0009392191064001073),\n",
       " ('to', 0.00040295500335795837)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
    "tmp_suggest5 = get_multiple_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 0.02189156832563708),\n",
       " ('you', 0.0035460992907801418),\n",
       " ('what', 6.719978496068813e-05),\n",
       " ('what', 6.719075455217362e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
    "tmp_suggest6 = get_multiple_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"'re\", 0.023579545454545454),\n",
       " ('?', 0.0026195153896529143),\n",
       " ('?', 0.00147245833612208),\n",
       " ('what', 6.715465717547512e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest7 = get_multiple_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('do', 0.009290814670561505),\n",
       " ('doing', 0.0017017934284592224),\n",
       " ('doing', 0.00046825874640444177),\n",
       " ('dl', 6.711859856366199e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest8 = get_multiple_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, starts_with=\"d\")\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
