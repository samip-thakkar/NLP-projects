{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common preprocessing steps for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thakk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the libraries\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
     ]
    }
   ],
   "source": [
    "#Change the corpus to lowercase\n",
    "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
    "corpus_lower = corpus.lower()\n",
    "\n",
    "print(corpus_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning makes me happy. i am happy because i am learning! \n"
     ]
    }
   ],
   "source": [
    "#Remove all the special character in the corpus\n",
    "corpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\n",
    "corpus_clean = re.sub(r\"[^A-Za-z0-9.?! ]+\", \"\", corpus)\n",
    "\n",
    "print(corpus_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
      "time parts = ['07', '33', '35']\n"
     ]
    }
   ],
   "source": [
    "#Get the date\n",
    "input_date = \"Sat May  9 07:33:35 CEST 2020\"\n",
    "\n",
    "#Split in date format\n",
    "date_ = input_date.split(' ')\n",
    "print(f\"date parts = {date_}\")\n",
    "\n",
    "#Split in time format\n",
    "time_ = date_[4].split(':')\n",
    "print(f\"time parts = {time_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
      "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the sentence into arrays of words\n",
    "sentence = 'i am happy because i am learning.'\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "print(tokenized_sentence)\n",
    "\n",
    "#Find the length of each word\n",
    "word_length =  [(word, len(word)) for word in tokenized_sentence]\n",
    "print(word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence to N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
      "\n",
      "[['i', 'am', 'happy'], ['am', 'happy', 'because'], ['happy', 'because', 'i'], ['because', 'i', 'am'], ['i', 'am', 'learning'], ['am', 'learning', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Define a function to covert the tokenized sentence into list of n-grams\n",
    "def sentence_to_ngram(tokenized_sentence, n):\n",
    "    l = len(tokenized_sentence)\n",
    "    n_grams = []\n",
    "    for i in range(l - n + 1):\n",
    "        n_grams += tokenized_sentence[i: i + n],\n",
    "    return n_grams\n",
    "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "\n",
    "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
    "n_gram = sentence_to_ngram(tokenized_sentence, 3)\n",
    "print(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "#Append start and end tags to a sentence\n",
    "start_tag = '<s>'\n",
    "end_tag = '</s>'\n",
    "\n",
    "def add_tags(tokenized_sentence, n):\n",
    "    tokenized_sentence = [start_tag] * (n - 1) + tokenized_sentence + [end_tag] \n",
    "    return tokenized_sentence\n",
    "\n",
    "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "tokenized_sentence_with_tags = add_tags(tokenized_sentence, 3)\n",
    "print(tokenized_sentence_with_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
