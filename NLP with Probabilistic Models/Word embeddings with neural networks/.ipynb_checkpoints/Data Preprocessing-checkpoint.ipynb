{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook deals with Data Preprocessing for word embeddings using Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from utils import get_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a sample sentence\n",
    "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the punctuations with the period for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: Who ❤️ \"word embeddings\" in 2020? I do!!!\n",
      "Cleaned data:  Who ❤️ \"word embeddings\" in 2020. I do.\n"
     ]
    }
   ],
   "source": [
    "#Clean the data\n",
    "print(f'Corpus: {corpus}')\n",
    "\n",
    "#Replace punctuations\n",
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "\n",
    "print(\"Cleaned data: \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization:  Who ❤️ \"word embeddings\" in 2020. I do.\n",
      "Tokenized data:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the data\n",
    "print(\"Before tokenization: \", data)\n",
    "\n",
    "tokenized_data = word_tokenize(data)\n",
    "\n",
    "print(\"Tokenized data: \", tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "#Convert the data to lower case\n",
    "data_cleaned = [ch.lower() for ch in tokenized_data\n",
    "               if ch.isalpha() or\n",
    "               ch == '.' or\n",
    "               emoji.get_emoji_regexp().search(ch)\n",
    "               ]\n",
    "\n",
    "print(\"Cleaned data: \", data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to perform all the cleaning and tokenization task\n",
    "def tokenize(corpus):\n",
    "    \n",
    "    data = re.sub(r'!,-;?]+', '.', corpus)\n",
    "    data_tokenized = word_tokenize(data)\n",
    "    data_cleaned = [ch.lower() for ch in data_tokenized\n",
    "                   if ch.isalpha() or\n",
    "                   ch == '.' or\n",
    "                   emoji.get_emoji_regexp().search(ch)\n",
    "                   ]\n",
    "    \n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because I am learning.\n",
      "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "#Test the function\n",
    "corpus = 'I am happy because I am learning'\n",
    "\n",
    "# Print new corpus\n",
    "print(f'Corpus:  {corpus}')\n",
    "\n",
    "# Save tokenized version of corpus into 'words' variable\n",
    "words = tokenize(corpus)\n",
    "\n",
    "# Print the tokenized version of the corpus\n",
    "print(f'Words (tokens):  {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  I love NLP!!! Thank you Coursera for this.. Do you have any other course??\n",
      "Pre processed data:  ['i', 'love', 'nlp', 'thank', 'you', 'coursera', 'for', 'do', 'you', 'have', 'any', 'other', 'course']\n"
     ]
    }
   ],
   "source": [
    "#Test the function onto custom sentence\n",
    "cust_sentence = \"I love NLP!!! Thank you Coursera for this.. Do you have any other course??\"\n",
    "\n",
    "print(\"Original sentence: \", cust_sentence)\n",
    "cust_sentence_processed = tokenize(cust_sentence)\n",
    "print(\"Pre processed data: \", cust_sentence_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window of word: Get the context of the centered word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function to get the windows\n",
    "def get_windows(words, c):\n",
    "    i = c\n",
    "    while i <= len(words) - c:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i-c):i] + words[i+1:i+c+1]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n",
      "['because', 'i', 'learning']\tam\n"
     ]
    }
   ],
   "source": [
    "#Test the function\n",
    "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'nlp']\tlove\n",
      "['love', 'thank']\tnlp\n",
      "['nlp', 'you']\tthank\n",
      "['thank', 'coursera']\tyou\n",
      "['you', 'for']\tcoursera\n",
      "['coursera', 'do']\tfor\n",
      "['for', 'you']\tdo\n",
      "['do', 'have']\tyou\n",
      "['you', 'any']\thave\n",
      "['have', 'other']\tany\n",
      "['any', 'course']\tother\n",
      "['other']\tcourse\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
    "for x, y in get_windows(tokenize(\"I love NLP!!! Thank you Coursera for this.. Do you have any other course??\"), 1):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the words into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to map the words to index and index to words\n",
    "\n",
    "def get_dict(data):\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    # return these correctly\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2Ind, Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the function\n",
    "word2Ind, Ind2word = get_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to index:  {'.': 0, 'am': 1, 'because': 2, 'happy': 3, 'i': 4, 'learning': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Word to index: \", word2Ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word:  {0: '.', 1: 'am', 2: 'because', 3: 'happy', 4: 'i', 5: 'learning'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index to word: \", Ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  6\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of vocabulary: \", len(word2Ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the one-hot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy:  3\n"
     ]
    }
   ],
   "source": [
    "print('happy: ', word2Ind['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot word vector of happy:  [0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Create an empty vector\n",
    "n = len(word2Ind)\n",
    "center_word_vector = np.zeros(n)\n",
    "\n",
    "idx = word2Ind['happy']\n",
    "center_word_vector[idx] = 1\n",
    "\n",
    "print(\"One-hot word vector of happy: \", center_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to get the one-hot word vector\n",
    "def word_to_one_hot_vector(word, word2Ind, n):\n",
    "    one_hot_vector = np.zeros(n)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the function\n",
    "word_to_one_hot_vector('happy', word2Ind, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting context word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of context words\n",
    "context_words = ['i', 'am', 'because', 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 1., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one-hot vectors for each context word using list comprehension\n",
    "context_words_vectors = [word_to_one_hot_vector(w, word2Ind, n) for w in context_words]\n",
    "\n",
    "# Print one-hot vectors for each context word\n",
    "context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the mean\n",
    "np.mean(context_words_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that will include the steps previously seen\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the function\n",
    "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words:  ['i', 'am', 'because', 'i'] -> [0.   0.25 0.25 0.   0.5  0.  ]\n",
      "Center word:  happy -> [0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words:  ['am', 'happy', 'i', 'am'] -> [0.   0.5  0.   0.25 0.25 0.  ]\n",
      "Center word:  because -> [0. 0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.   0.25 0.25 0.25 0.   0.25]\n",
      "Center word:  i -> [0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Context words:  ['because', 'i', 'learning', '.'] -> [0.25 0.   0.25 0.   0.25 0.25]\n",
      "Center word:  am -> [0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['i', 'am', '.'] -> [0.33333333 0.33333333 0.         0.         0.33333333 0.        ]\n",
      "Center word:  learning -> [0. 0. 0. 0. 0. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus\n",
    "for context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n",
    "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, n)}')\n",
    "    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, n)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator function 'get_training_example'\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.   0.25 0.25 0.   0.5  0.  ]\n",
      "Center word vector:  [0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.   0.5  0.   0.25 0.25 0.  ]\n",
      "Center word vector:  [0. 0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.   0.25 0.25 0.25 0.   0.25]\n",
      "Center word vector:  [0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Context words vector:  [0.25 0.   0.25 0.   0.25 0.25]\n",
      "Center word vector:  [0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.33333333 0.33333333 0.         0.         0.33333333 0.        ]\n",
      "Center word vector:  [0. 0. 0. 0. 0. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus using the generator function\n",
    "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, n):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
