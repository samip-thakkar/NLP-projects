{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation and Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import gensim\n",
    "import nltk\n",
    "import scipy\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "\n",
    "from utils import (cosine_similarity, get_dict, process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings fotr English and French Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of English embeddings subset dictionary:  6370\n",
      "Length of French embeddings subset dictionary:  5766\n"
     ]
    }
   ],
   "source": [
    "#Get the English and French subsets\n",
    "en_embeddings_subset = pickle.load(open('en_embeddings.p', 'rb'))\n",
    "fr_embeddings_subset = pickle.load(open('fr_embeddings.p', 'rb'))\n",
    "print(\"Length of English embeddings subset dictionary: \", len(en_embeddings_subset))\n",
    "print(\"Length of French embeddings subset dictionary: \", len(fr_embeddings_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dictionary:  5000\n",
      "Length of testing dictionary:  1500\n"
     ]
    }
   ],
   "source": [
    "#Load the train and test dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "\n",
    "print(\"Length of training dictionary: \", len(en_fr_train))\n",
    "print(\"Length of testing dictionary: \", len(en_fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the embedding and Transform matrix\n",
    "\n",
    "#Define the function get_matrices to get the vector embedding for every English and French word\n",
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \n",
    "    #Create lists for storing English and France Embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "    \n",
    "    #Get the set of English and French words\n",
    "    english_set = english_vecs.keys()\n",
    "    french_set = french_vecs.keys()\n",
    "    \n",
    "    #Store the fench words that are part of en-fr dictionary\n",
    "    french_words = set(en_fr.values())\n",
    "    \n",
    "    #Iterate through all the english and french words in en_fr dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        \n",
    "        #Check if both words have embeddings\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "            \n",
    "            #Get the English and French embeddings\n",
    "            en_vec = english_vecs[en_word]\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            \n",
    "            #Add french and english words to their lists\n",
    "            X_l.append(en_vec)\n",
    "            Y_l.append(fr_vec)\n",
    "            \n",
    "    #Stack the vectors of X_l in matrix X, and Y_l to Y\n",
    "    X = np.stack(X_l)\n",
    "    Y = np.stack(Y_l)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the X_train and Y_train\n",
    "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translation as linear transformation of embeddings (Using R matrix)\n",
    "#Define a function to compute loss for R matrix\n",
    "def compute_loss(X, Y, R):\n",
    "    \n",
    "    #Get the difference\n",
    "    diff = np.dot(X, R) - Y\n",
    "    \n",
    "    #Square the differnce\n",
    "    diff_squared = diff ** 2\n",
    "    \n",
    "    #Get the sum of squared difference \n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    \n",
    "    #Calculate the loss\n",
    "    loss = sum_diff_squared / len(X)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to compute gradient loss wrt R\n",
    "def compute_gradient(X, Y, R):\n",
    "    \n",
    "    gradient = np.dot(X.T, (np.dot(X, R) - Y)) * 2 / len(X)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal R with gradient descent\n",
    "def align_embeddings(X, Y, train_steps = 100, learning_rate = 0.0003):\n",
    "    np.random.seed(129)\n",
    "    \n",
    "    #Create random R\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "    \n",
    "    #Iterate over number of epochs\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "        \n",
    "        #Update R\n",
    "        R -= learning_rate * gradient\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.7242\n",
      "loss at iteration 25 is: 3.6283\n",
      "loss at iteration 50 is: 3.5350\n",
      "loss at iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "#Test the functions\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "#Calculate R\n",
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to find Nearest Neighbor\n",
    "def nearest_neighbor(v, candidates, k = 1):\n",
    "    #Define a list to store similarity\n",
    "    similarity_l = []\n",
    "    \n",
    "    #For each candidate vector\n",
    "    for row in candidates:\n",
    "        \n",
    "        #Get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "        \n",
    "        #Append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    #Sort the similarity list and get the indices\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    #Get the last k most similar vetors (descending)\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Test the knn\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the vocabulary\n",
    "def test_vocabulary(X, Y, R):\n",
    "    \n",
    "    #Get the prediction\n",
    "    pred = np.dot(X, R)\n",
    "    \n",
    "    #Get the counter of correct number of predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    #Loop through each row in pred\n",
    "    for i in range(len(pred)):\n",
    "        \n",
    "        #Get the nearest neighbor's index\n",
    "        pred_idx = nearest_neighbor(pred[i], Y)\n",
    "        \n",
    "        #Increment counter if correct predicrtion\n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "        \n",
    "    #Calculate the accuracy\n",
    "    accuracy = num_correct / len(pred)\n",
    "    \n",
    "    return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "#Calculate the accuracy\n",
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH and Document Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = positive_tweets + negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the documents embeddings\n",
    "\n",
    "#Define a function to get the embedding of the whole document\n",
    "def get_document_embedding(tweet, en_embeddings):\n",
    "    \n",
    "    #Create empty array of 300 dim to store document embedding\n",
    "    document_embedding = np.zeros(300)\n",
    "    \n",
    "    #Get the processed tweet \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    \n",
    "    #Iterate over all the words and get the embeddings of each word and add to document embeddding\n",
    "    for word in processed_tweet:\n",
    "        if word in en_embeddings:\n",
    "            document_embedding += en_embeddings[word]\n",
    "    \n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the function on a custom tweet\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to get all the vectors of a document\n",
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    \n",
    "    #Create a dictionary that has tweet index as key and corresponding document embedding as value\n",
    "    ind2doc_dict = {}\n",
    "    \n",
    "    #Create a list to store alll the document vectors\n",
    "    document_vectors_l = []\n",
    "    \n",
    "    #Iterate over all the documents\n",
    "    for i, doc in enumerate(all_docs):\n",
    "        \n",
    "        #Get the embeddings of document\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings_subset)\n",
    "        \n",
    "        #Save the doc embedding in dictionary\n",
    "        ind2doc_dict[i] = doc_embedding\n",
    "        \n",
    "        #Appenf the doc_embedding in the list\n",
    "        document_vectors_l.append(doc_embedding)\n",
    "        \n",
    "    #Convert the list to matrix\n",
    "    document_vec_matrix = np.stack(document_vectors_l)\n",
    "    \n",
    "    return document_vec_matrix, ind2doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the embeddings for all tweets\n",
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "#Print the shape of list and dictionary\n",
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@PetiteMistress DO IT! I want to start one for making small games, but I feel like I need to get a jump start before asking for support :(\n"
     ]
    }
   ],
   "source": [
    "#Find the most similar tweet from the corpus using cosine similarity\n",
    "my_tweet = 'i am very happy, I am learning'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)\n",
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the most similar tweets using LSH (Instead of searching in 10000 tweets, search only in similar regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "#Get the number of dimensions and length of vector\n",
    "N_VECS = len(all_tweets)       # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each plane has 2 bunckets, so n will have 2^n buckets, we need 16 vectors in each region, so will have 10 planes. (2 ^ 10 * 16)\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the planes list\n",
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size = (N_DIMS, N_PLANES)) for _ in range(N_UNIVERSE)]\n",
    "len(planes_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to get a hash value of a vector wrt multi planes\n",
    "def hash_value_of_vector(v, planes):\n",
    "    \n",
    "    #Get the dot product of vector and plane\n",
    "    dot_product = np.dot(v, planes)\n",
    "    \n",
    "    #Get the sign\n",
    "    dot_product_sign = np.sign(dot_product)\n",
    "    \n",
    "    #Get the scalar sign\n",
    "    h = dot_product_sign > 0\n",
    "    \n",
    "    #Convert 2d array to 1d\n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    #Initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "    \n",
    "    #Get the number of planes\n",
    "    n_planes = planes.shape[1]\n",
    "    \n",
    "    #Iterate over all planes and calculate the hash value\n",
    "    for i in range(n_planes):\n",
    "        hash_value += 2 **i * h[i]\n",
    "    \n",
    "    #Create value to integer\n",
    "    hash_value = int(hash_value)\n",
    "    \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The hash value for this vector, and the set of planes at index 0, is 768\n"
     ]
    }
   ],
   "source": [
    "#Test the function\n",
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(f\" The hash value for this vector,\",\n",
    "      f\"and the set of planes at index {idx},\",\n",
    "      f\"is {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the hash buckets\n",
    "def make_hash_table(vecs, planes):\n",
    "    #Get the number of planes\n",
    "    n_planes = planes.shape[1]\n",
    "    \n",
    "    #Get the number of buckets\n",
    "    n_buckets = 2 ** n_planes\n",
    "    \n",
    "    #Create an empty hash table and id_table dictionary\n",
    "    hash_table = {i:[] for i in range(n_buckets)}\n",
    "    id_table = {i:[] for i in range(n_buckets)}\n",
    "    \n",
    "    #Iterate over each vector and place in proper bucket\n",
    "    for i, v in enumerate(vecs):\n",
    "        \n",
    "        #Calculate the hash value\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        \n",
    "        #Append in the dictionary\n",
    "        hash_table[hash_value].append(v)\n",
    "        \n",
    "        #Append the index in id_table\n",
    "        id_table[hash_value].append(i)\n",
    "        \n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 1351 document vectors\n",
      "The id table at key 0 has 1351\n",
      "The first 5 document indices stored at key 0 of are [3, 8, 16, 18, 29]\n"
     ]
    }
   ],
   "source": [
    "#Test the function\n",
    "np.random.seed(0)\n",
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "#Create all the hash tables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "\n",
    "for universe_id in range(N_UNIVERSE):\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate KNN using LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to get approximate knn\n",
    "def approximate_knn(doc_id, v, planes_l, k = 1, num_universes_to_use = N_UNIVERSE):\n",
    "    \n",
    "    assert num_universes_to_use <= N_UNIVERSE\n",
    "    \n",
    "    #Create the list for vectors to be considered\n",
    "    vecs_to_consider = list()\n",
    "    \n",
    "    #List of documents id\n",
    "    ids_to_consider = list()\n",
    "    \n",
    "    #Set for ids to consider\n",
    "    ids_to_consider_set = set()\n",
    "    \n",
    "    #Loop through universe of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "        \n",
    "        #Get the set of planes from list\n",
    "        planes = planes_l[universe_id]\n",
    "        \n",
    "        #Get the hash value for the vector for set of these planes\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        \n",
    "        #Get the hash table for this universe_id\n",
    "        hash_table = hash_tables[universe_id]\n",
    "        \n",
    "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # get the id_table for this particular universe_id\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "        # remove the id of the document that we're searching\n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)\n",
    "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider.append(document_vector_at_i)\n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                ids_to_consider.append(new_id)\n",
    "\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "Fast considering 77 vecs\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 2140\n",
      "document contents: @PopsRamjet come one, every now and then is not so bad :)\n",
      "Nearest neighbor at document id 701\n",
      "document contents: With the top cutie of Bohol :) https://t.co/Jh7F6U46UB\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
